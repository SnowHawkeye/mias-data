{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing a dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd                                     # for dataset manipulation (DataFrames)\n",
    "import numpy as np                                      # allows some mathematical operations\n",
    "import matplotlib.pyplot as plt                         # library used to display graphs\n",
    "import seaborn as sns                                   # more convenient visualisation library for dataframes\n",
    "import missingno as msno                                # utility library for missing values\n",
    "from sklearn.model_selection import train_test_split    # for classification\n",
    "from sklearn.neighbors import KNeighborsClassifier      # for classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes_data.csv\")\n",
    "data_columns = list(df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "### Preliminary analysis\n",
    "\n",
    "Using what we learned last time, familiarize yourself with the diabetes dataset.\n",
    "What observations can you make ? What problem do you think we are trying to solve with this data ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your answers here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Objectives and baseline method\n",
    "For the rest of this practical, we will use a K-nearest-neighbours (KNN) algorithm to make a diagnosis from the dataset. This means that today, the only way we can improve this diagnosis is by pre-processing the data.\n",
    "The accuracy of the algorithm will be our metric to evaluate the quality of our pre-processing.\n",
    "\n",
    "Below is the function you will need to use the algorithm. You don't need to understand it yet - it will all be explained in future lessons !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    X = data.drop(\"Outcome\", axis=1)\n",
    "    y = data.Outcome\n",
    "    return train_test_split(X, y,\n",
    "                            test_size=0.33,  # 10% of the data will be used for testing\n",
    "                            random_state=42,  # ensures reproducibility of the test\n",
    "                            stratify=y  # ensures the proportion of ill people is the same\n",
    "                            )\n",
    "\n",
    "def print_knn_score(scores, data_type=\"\"):\n",
    "    max_score = max(scores)\n",
    "    k_values_max_score = [i + 1 for i, v in enumerate(scores) if v == max_score]\n",
    "    print(f'Max {data_type} score {max_score * 100} % for k = {[i for i in k_values_max_score]}')\n",
    "\n",
    "def diagnosis_knn(data):\n",
    "    \"\"\" KNN-based classification for diabetes diagnosis. \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_data(data)\n",
    "    test_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    for k in range(1, 15):\n",
    "        knn = KNeighborsClassifier(k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        train_scores.append(knn.score(X_train, y_train))  # \"score\" for KNN is the accuracy of the classification\n",
    "        test_scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "    print_knn_score(train_scores, \"train\")\n",
    "    print_knn_score(test_scores, \"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this, we can have a \"baseline\" score, *i.e.* a score we can refer to in the future to see how performance evolves with our changes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diagnosis_knn(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaling the data\n",
    "\n",
    "KNN is a distance-based algorithm, meaning it is can sensitive to the absolute values of the features. One possible way to counter this issue is to standardize the data (subtract mean and divide by standard deviation).\n",
    "\n",
    "Be careful not to mix up **normalization** and **standardization**!\n",
    "**Normalizing data** means dividing each value by its norm. There are several norms (l1, l2), which all have different properties.\n",
    "\n",
    "**Normalization changes the distribution of the data**, whereas **standardization simply changes its scale**.\n",
    "\n",
    "#### Task\n",
    "Apply each of the following operations. For each of them, check how the prediction score changes, and try to interpret it.\n",
    "- Standardize the data.\n",
    "- Normalize the data. Make sure to use the original data, and not the standardized data!\n",
    "- In a separate variable, use both standardization and normalization.\n",
    "- *Bonus* : You can check out the [`scikit-learn` API](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) and use one of the other scalers listed there, such as `MinMaxScaler` or `MaxAbsScaler`. Think of use cases for these scalers.\n",
    "\n",
    "*Hints*:\n",
    "- You can either try to define these functions yourself, or you can use `scikit-learn`'s dedicated objects : [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [`Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer). Refer to the documentation for information on how to use them. Don't forget the imports!\n",
    "- It is advised to create a function that will do the scaling for you. This way, you can easily scale other datasets in the rest of the practical."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example for StandardScaler ----------------------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()                                   # create an instance of the scaler\n",
    "outcome = df[\"Outcome\"].to_numpy()                          # we use to_numpy() to avoid problems with the index\n",
    "df_std = df.drop([\"Outcome\"], axis=1, inplace=False)        # create a copy excluding target\n",
    "df_std = scaler.fit_transform(df_std)                       # fit the scaler\n",
    "df_std = pd.DataFrame(df_std, columns=data_columns[:-1])    # transform it to dataframe\n",
    "df_std[\"Outcome\"] = outcome                                 # add the outcome column back\n",
    "# -------------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scale(data):\n",
    "    scaled_data = ...\n",
    "    ...                        # perform some pre-processing here\n",
    "    return scaled_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Managing missing values\n",
    "\n",
    "#### \"What missing values?\"\n",
    "\n",
    "Using the functions we saw in the last practical, determine (if you have not already) whether the dataset contains missing values.\n",
    "Now observe the minimum and maximum values for each feature. Does anything seem weird to you? Try to interpret the abnormalities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your answers here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cleaning the data\n",
    "\n",
    "You must now have noticed that there are indeed missing values in the dataset. To avoid creating bias in the data and misleading both analysis and algorithms, it is actually better to replace those values by actual missing values, which we will manage later on.\n",
    "\n",
    "In the code cell below, replace the missing values with `NaN` types, meaning \"Not a Number\".\n",
    "Make sure to use the original data, not the scaled data from the previous section!\n",
    "It is a good idea to scale data, but the scaling needs to be done after the cleaning. In order to compare the results with and without scaled data, you will need to re-scale the data everytime we impute a different value for `NaN`s.\n",
    "\n",
    "*Hints*:\n",
    "- Make sure to only replace values in the relevant columns!\n",
    "- You can use the `replace()` function to perform the replacement\n",
    "- You can indicate a `NaN` type with `np.NaN`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This syntax lets you select several columns at once\n",
    "# df[['column1','column2','column3']]\n",
    "\n",
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observe missing data\n",
    "\n",
    "The `missingno` library lets you visualize where `NaN`s are in your dataset. Sometimes, it lets you determine whether your data is missing randomly or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deleting rows with missing values\n",
    "\n",
    "As a first approach, try deleting rows where data is missing. How much data do you have left?\n",
    "Check the new score. How do you interpret this result?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replacing missing values\n",
    "\n",
    "In a lot of cases, replacing the missing data can provide better classification results than simply deleting the data. In the sections below, we will test different methods for data replacement.\n",
    "\n",
    "For each of these methods :\n",
    "- Perform the replacement on the data we cleaned up\n",
    "- Check how the diagnosis score evolves\n",
    "- Try providing an explanation for this evolution\n",
    "- Scale the data and see if there are any changes\n",
    "\n",
    "*Hint: Pandas has a `fillna()` function!*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Replacing with mean values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Replacing with median values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Replacing with random values\n",
    "\n",
    "Here are some ways you can replace with random values:\n",
    "- Replace with completely random values\n",
    "- Replace with random values drawn uniformly between the minimum and maximum of each column\n",
    "- Replace with random values drawn from a certain distribution for each column\n",
    "\n",
    "*Hints*:\n",
    "  - Observing the data with a `displot` (with `kind=\"kde\"`) can help you guess what type of distribution you need\n",
    "  - You can use functions such as [`numpy.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) to draw random samples. Numpy has functions for many distributions, such as uniform, normal or exponential."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scikit learn imputation\n",
    "\n",
    "For this step, you can try using `scikit-learn`'s `SimpleImputer`, `KNN-Imputer` and / or `IterativeImputer`. Make sure you refer to the documentation to parametrize the imputers!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mixing and matching\n",
    "\n",
    "You are not obligated to use the same pre-processing for every column. In the cell below, try combining different methods in order to get the best classification possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
